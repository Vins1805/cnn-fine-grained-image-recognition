{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from datasets.dataset import CUBDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from numpy import unravel_index\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_parts = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CUBDataset()\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=10, shuffle=True)\n",
    "testset = CUBDataset(is_test = True)\n",
    "testloader = DataLoader(dataset=testset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Vincent/.cache\\torch\\hub\\pytorch_vision_v0.9.0\n"
     ]
    }
   ],
   "source": [
    "vgg19 = torch.hub.load('pytorch/vision:v0.9.0', 'vgg19', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_map(feature_channels, d):\n",
    "    \"\"\"\n",
    "    i = n-th Part\n",
    "    j = 1..c\n",
    "    Mi(X) = sigmoid(∑dji[W∗X]j)\n",
    "    W∗X = j-th feature channel\n",
    "    dji = j-th weight vector\n",
    "    \"\"\"\n",
    "    M = 0\n",
    "    for i in range(feature_channels.size(0)):\n",
    "        M += d[i] * feature_channels[i]\n",
    "    torch.sigmoid(M)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss_CNG(M, t):\n",
    "    \"\"\"\n",
    "    Input are the coordinates of the position vector \"t\" and all attention maps of one image \"M\".\n",
    "    Lcng(Mi) =Dis(Mi) +λDiv(Mi)\n",
    "    The λ in Eqn. (7) and mrg in Eqn. (9) are empirically set to 2 and 0.02.\n",
    "    \"\"\"\n",
    "    weight = 2\n",
    "    margin = .02\n",
    "    loss = list()\n",
    "    \n",
    "    def distance(mi, ix, iy, i, t):\n",
    "        \"\"\"Dis(Mi) =∑(x,y)∈Mi(mi(x, y)[||x−tx||2+||y−ty||2])\"\"\"\n",
    "        return mi[iy,ix] * ((ix - t[i][0])** 2 + (iy - t[i][1])**2)\n",
    "\n",
    "    def diversity(mi, ix, iy, mrg):\n",
    "        \"\"\"Div(Mi) =∑(x,y)∈Mi(mi(x, y)[max(k/=im)_k(x, y)−mrg])\"\"\"\n",
    "        return mi[iy,ix] * (max([mk[iy,ix] for mk in M]) - mrg)\n",
    "    \n",
    "    for i, mi in enumerate(M):\n",
    "        dis = 0\n",
    "        div = 0\n",
    "        for iy in range(mi.size(0)):\n",
    "            for ix in range(mi.size(1)):\n",
    "                dis += distance(mi, ix, iy, i, t)\n",
    "                div += diversity(mi, ix, iy, margin)\n",
    "        loss.append(dis + weight * div)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    FC Layers which produce a weight vector d_i(X) from [d_1 .. d_c], where c is the length of feature channels.\n",
    "    Takes as input convolutional features which gets represented as positional vectors t.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        t = position vector\n",
    "        d = weight vector\n",
    "        \"\"\"\n",
    "        d = self.fc1(t)\n",
    "        d = self.fc2(d)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# channel grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 448, 448])\n"
     ]
    }
   ],
   "source": [
    "# TODO add to preprocessing\n",
    "img , label = trainset[120]\n",
    "img = transforms.ToTensor()(img)\n",
    "img = img.permute(1,2,0)\n",
    "img = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(img/255) \n",
    "img = img.unsqueeze(0)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate position vector \"t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = vgg19.features(img)\n",
    "\n",
    "coordinates = list()\n",
    "for channel in channels.detach().numpy()[0]:\n",
    "    coordinates = coordinates + [unravel_index(channel.argmax(), channel.shape)]\n",
    "\n",
    "t_flat = [point for coordinate in coordinates for point in coordinate]\n",
    "t_flat = torch.FloatTensor(t_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate N-times neural networks (Part FC's)\n",
    "NNs = {N: Net() for N in range(N_parts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = list()\n",
    "for N in range(N_parts):\n",
    "    \n",
    "    # Calculate weight vector \"d\"\n",
    "    d = NNs[N](t_flat)\n",
    "\n",
    "    # Calculate attention map \"M\"\n",
    "    mi = attention_map(channels[0], d)\n",
    "    M.append(mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Loss CNG of all Mi's \"L_cng(M_i)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-277369.0312, grad_fn=<AddBackward0>),\n",
       " tensor(1068108.5000, grad_fn=<AddBackward0>),\n",
       " tensor(63077.7266, grad_fn=<AddBackward0>),\n",
       " tensor(881210.5000, grad_fn=<AddBackward0>),\n",
       " tensor(-249701.1875, grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_loss_cng = Loss_CNG(M, coordinates)\n",
    "M_loss_cng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "input_channels = 1\n",
    "output_features = 6\n",
    "epoch = 1\n",
    "save_model_name = 'models/pretrained.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CUBDataset()\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=10, shuffle=True)\n",
    "testset = CUBDataset(is_test = True)\n",
    "testloader = DataLoader(dataset=testset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = torch.hub.load('pytorch/vision:v0.9.0', 'vgg19', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img , label = trainset[120]\n",
    "print(img.shape)\n",
    "print(img.transpose(1,2,0).shape)\n",
    "baseimage = Image.fromarray(img.transpose(1,2,0).astype(np.uint8))\n",
    "baseimage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.ToTensor()(img)\n",
    "img = img.permute(1,2,0)\n",
    "mean, std = img.mean([1,2]), img.std([1,2])\n",
    "#print(\"Mean:\", mean, \"\\nStd:\", std)\n",
    "#print(\"image shape:\", img.shape)\n",
    "#img = transforms.Normalize(mean, std)(img)\n",
    "#/255 -> Vgg19 Normalization\n",
    "img = img/255\n",
    "img = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(img) \n",
    "img = img.unsqueeze(0)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = nn.Softmax(dim=1)(vgg19(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"classes_vgg19.txt\",\"r\") as f:\n",
    "    classes = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_o = 0\n",
    "highscore = None\n",
    "for i, o in enumerate(output[0]):\n",
    "    if max_o < o:\n",
    "        highscore = i\n",
    "        max_o = o\n",
    "\n",
    "print(highscore,classes[highscore])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = vgg19.features(img)\n",
    "channels = channels.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square = 5\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(channels[0, ix-1, :, :], cmap='gray')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula 1: determine t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = list()\n",
    "for channel in channels[0]:\n",
    "    #print(np.max(channel))\n",
    "    t.append(unravel_index(channel.argmax(), channel.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = zip(*t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = list()\n",
    "y_list = list()\n",
    "for x,y in t:\n",
    "    x_list.append(x)\n",
    "    y_list.append(y)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_list,y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_xy = counter.most_common(20)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for xy,_ in part_xy:\n",
    "    x,y = xy\n",
    "    i = ImageDraw.Draw(baseimage).rectangle([(x-1)*32,(y-1)*32,x*32,y*32])\n",
    "baseimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = ImageDraw.Draw(baseimage)\n",
    "i.rectangle([1,1,32,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clustering(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Clustering, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 512)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_flatten = [i for xy in t for i in xy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_flatten= torch.Tensor(t_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19.fc1 = nn.Linear(in_features=1024, out_features=512, bias=True)\n",
    "vgg19.fc2 = nn.Linear(in_features=512, out_features=512, bias=True)\n",
    "#vgg19.fc(torch.Tensor(t).permute(1,0))\n",
    "vgg19.fc2(vgg19.fc1(t_flatten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Part(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Part, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_matrix = torch.clone(x)\n",
    "        conv_matrix = conv_matrix.reshape(conv_matrix.size(0), 512, 1, 784) #512 = patterns; 784 = 28x28 pattern w x h\n",
    "        conv_matrix = conv_matrix.transpose(1, 3)\n",
    "        x = F.avg_pool2d(x, kernel_size=28, stride=28)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x).unsqueeze(1).unsqueeze(1)\n",
    "        x = F.interpolate(x, (1, 784), mode='bilinear', align_corners=True)\n",
    "        x = x.squeeze(1).squeeze(1).unsqueeze(2).unsqueeze(3)\n",
    "        x = x * conv_matrix\n",
    "        x = F.avg_pool2d(x, kernel_size=(1, 512), stride=512)\n",
    "        x = x * 0.1\n",
    "        x = F.softmax(x, dim=1)\n",
    "        x = torch.exp(x)\n",
    "        x = x + 1\n",
    "        x = torch.log(x)\n",
    "        x = x * 4\n",
    "        x = x.squeeze(2).squeeze(2)\n",
    "        return x.reshape(x.size(0), 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Loss, self).__init__()\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        loss_sum = torch.zeros(1).cuda()\n",
    "        indexes = Loss.get_max_index(tensor)\n",
    "        for i in range(len(indexes)):\n",
    "            max_x, max_y = indexes[i]\n",
    "            for j in range(tensor.size(1)):\n",
    "                for k in range(tensor.size(2)):\n",
    "                    loss_sum += ((max_x - j) * (max_x - j) + (max_y - k) * (max_y - k)) * tensor[i, j, k]\n",
    "        return loss_sum\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_index(tensor):\n",
    "        shape = tensor.shape\n",
    "        indexes = []\n",
    "        for i in range(shape[0]):\n",
    "            mx = tensor[i, 0, 0]\n",
    "            x, y = 0, 0\n",
    "            for j in range(shape[1]):\n",
    "                for k in range(shape[2]):\n",
    "                    if tensor[i, j, k] > mx:\n",
    "                        mx = tensor[i, j, k]\n",
    "                        x, y = j, k\n",
    "            indexes.append([x, y])\n",
    "        return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = Part()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(part.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img , label = trainset[55]\n",
    "Image.fromarray(img.transpose(1,2,0).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.ToTensor()(img)\n",
    "img = img.permute(1,2,0)\n",
    "print(img.shape)\n",
    "mean, std = img.mean([1,2]), img.std([1,2])\n",
    "print(\"Mean:\", mean, \"\\nStd:\", std)\n",
    "img = img/255\n",
    "img = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(img)\n",
    "img = img.unsqueeze(0)\n",
    "print(\"image shape:\", img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = vgg19.features[0:36](img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"channel shape:\", channels.shape)\n",
    "output = part(channels)\n",
    "print(\"output shape:\", output.shape)\n",
    "optimizer.zero_grad()\n",
    "loss = loss_fn(output)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_model = torch.load(save_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = part_model(channels)\n",
    "x = output.permute(1,2,0).detach().numpy()\n",
    "print(x.shape)\n",
    "print(type(x))\n",
    "print(np.stack((x,x,x),axis=2).squeeze(-1).shape)\n",
    "Image.fromarray(np.stack((x,x,x),axis=2).squeeze(-1).astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "for epoch_number in range(epoch):\n",
    "    running_loss, count, acc = 0., 0, 0.\n",
    "    for batch, label in trainloader:\n",
    "        for img in batch:\n",
    "            t = time.time()\n",
    "            #print(img.shape)\n",
    "            img = img/255\n",
    "            img = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(img)\n",
    "            img = img.unsqueeze(0)\n",
    "            #print(\"image shape:\", img.shape)\n",
    "            channels = vgg19.features[0:36](img)\n",
    "            #print(\"channel shape:\", channels.shape)\n",
    "            output = part(channels)\n",
    "            #print(\"output shape:\", output.shape)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            count += img.size(0)\n",
    "            #print(time.time() - t)\n",
    "        print(epoch_number, count, running_loss, Loss.get_max_index(output))\n",
    "\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(part, save_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
