{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from datasets.dataset import CUBDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from numpy import unravel_index\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_parts = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CUBDataset()\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=10, shuffle=True)\n",
    "testset = CUBDataset(is_test = True)\n",
    "testloader = DataLoader(dataset=testset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Vincent/.cache\\torch\\hub\\pytorch_vision_v0.9.0\n"
     ]
    }
   ],
   "source": [
    "vgg19 = torch.hub.load('pytorch/vision:v0.9.0', 'vgg19', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_based_features(feature_channels, M):\n",
    "    \"\"\"\n",
    "    Pi(X) =c∑j=1([W∗X]j·Mi)\n",
    "    \"\"\"\n",
    "    P = list()\n",
    "    for mi in M:\n",
    "        pi = sum(feature_channel @ mi for feature_channel in feature_channels)\n",
    "        P.append(pi.unsqueeze(0))\n",
    "    return torch.cat(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_map(feature_channels, d):\n",
    "    \"\"\"\n",
    "    i = n-th Part\n",
    "    j = 1..c\n",
    "    Mi(X) = sigmoid(∑dji[W∗X]j)\n",
    "    W∗X = j-th feature channel\n",
    "    dji = j-th weight vector\n",
    "    \"\"\"\n",
    "    M = 0\n",
    "    for i in range(feature_channels.size(0)):\n",
    "        M += d[i] * feature_channels[i]\n",
    "    M = torch.sigmoid(M)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_CNG(M, t):\n",
    "    \"\"\"\n",
    "    Input are the coordinates of the position vector \"t\" and all attention maps of one image \"M\".\n",
    "    Lcng(Mi) =Dis(Mi) +λDiv(Mi)\n",
    "    The λ in Eqn. (7) and mrg in Eqn. (9) are empirically set to 2 and 0.02.\n",
    "    \"\"\"\n",
    "    weight = 2\n",
    "    margin = .02\n",
    "    loss = list()\n",
    "    \n",
    "    def distance(mi, ix, iy, i, t):\n",
    "        \"\"\"Dis(Mi) =∑(x,y)∈Mi(mi(x, y)[||x−tx||2+||y−ty||2])\"\"\"\n",
    "        tx, ty = list(zip(*t))\n",
    "        tx, ty = np.array(tx), np.array(ty)\n",
    "        euclidean_norm = lambda x: np.sqrt(sum(elm**2 for elm in x))\n",
    "        return mi[iy,ix] * (euclidean_norm(ix - tx)** 2 + euclidean_norm(iy - ty)**2)\n",
    "\n",
    "    def diversity(M, i, ix, iy, mrg):\n",
    "        \"\"\"Div(Mi) =∑(x,y)∈Mi(mi(x, y)[max(k/=im)_k(x, y)−mrg])\"\"\"\n",
    "        return M[i][iy,ix] * (max([M[k][iy,ix] for k in range(len(M)) if k != i]) - mrg)\n",
    "    \n",
    "    for i, mi in enumerate(M):\n",
    "        dis = 0\n",
    "        div = 0\n",
    "        for iy in range(mi.size(0)):\n",
    "            for ix in range(mi.size(1)):\n",
    "                dis += distance(mi, ix, iy, i, t)\n",
    "                div += diversity(M, i, ix, iy, margin)\n",
    "        loss.append(dis + weight * div)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    FC Layers which produce a weight vector d_i(X) from [d_1 .. d_c], where c is the length of feature channels.\n",
    "    Takes as input convolutional features which gets represented as positional vectors t.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        t = position vector\n",
    "        d = weight vector\n",
    "        \"\"\"\n",
    "        d = self.fc1(t)\n",
    "        d = self.fc2(d)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# channel grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 448, 448])\n"
     ]
    }
   ],
   "source": [
    "# TODO add to preprocessing\n",
    "img , label = trainset[120]\n",
    "img = transforms.ToTensor()(img)\n",
    "img = img.permute(1,2,0)\n",
    "img = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(img/255) \n",
    "img = img.unsqueeze(0)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate position vector \"t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = vgg19.features(img)\n",
    "\n",
    "coordinates = list()\n",
    "for channel in channels.detach().numpy()[0]:\n",
    "    coordinates = coordinates + [unravel_index(channel.argmax(), channel.shape)]\n",
    "\n",
    "t_flat = [point for coordinate in coordinates for point in coordinate]\n",
    "t_flat = torch.FloatTensor(t_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate N-times neural networks (Part FC's)\n",
    "NNs = {N: Net() for N in range(N_parts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = list()\n",
    "for N in range(N_parts):\n",
    "    \n",
    "    # Calculate weight vector \"d\"\n",
    "    d = NNs[N](t_flat)\n",
    "\n",
    "    # Calculate attention map \"M\"\n",
    "    mi = attention_map(channels[0], d)\n",
    "    M.append(mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Loss CNG of all Mi's \"L_cng(M_i)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(4574378.5000, grad_fn=<AddBackward0>),\n",
       " tensor(5447335.5000, grad_fn=<AddBackward0>),\n",
       " tensor(954204.8750, grad_fn=<AddBackward0>),\n",
       " tensor(3771872.2500, grad_fn=<AddBackward0>),\n",
       " tensor(2211722.5000, grad_fn=<AddBackward0>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_loss_cng = loss_CNG(M, coordinates)\n",
    "M_loss_cng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 14, 14])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = part_based_features(channels[0], M)\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_clf = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19.classifier[0] = nn.Linear(in_features=980, out_features=4096, bias=True)\n",
    "vgg19.classifier[6] = nn.Linear(in_features=4096, out_features=200, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax(dim=0)(vgg19.classifier(P.view(980))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNs = {N: Net() for N in range(N_parts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "torch.Size([5])\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "torch.Size([5])\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "torch.Size([5])\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "torch.Size([5])\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "torch.Size([5])\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "torch.Size([5])\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "torch.Size([5])\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "torch.Size([5])\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "torch.Size([5])\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "optimizers = dict()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    M = list()\n",
    "    for N in range(N_parts):      \n",
    "        # Calculate weight vector \"d\"\n",
    "        d = NNs[N](t_flat)\n",
    "        optimizers[N] = torch.optim.Adam(NNs[N].parameters(), lr = 0.01)\n",
    "\n",
    "        # Calculate attention map \"M\"\n",
    "        mi = attention_map(channels[0], d)\n",
    "        M.append(mi)\n",
    "        \n",
    "    l_cng = torch.tensor(loss_CNG(M, coordinates), requires_grad = True)\n",
    "\n",
    "    for i, l in enumerate(l_cng):\n",
    "        l.backward(retain_graph=True)\n",
    "    \n",
    "    for N in range(N_parts): \n",
    "        optimizers[N].step()\n",
    "        optimizers[N].zero_grad()\n",
    "        \n",
    "    print(loss,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n",
      "tensor([2225172.2500, 3401613.2500, 4432599.5000, 2114707.2500, 3412832.5000],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-ea28e9037c4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml_cng\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_clf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mN\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_parts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "optimizers = dict()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    M = list()\n",
    "    for N in range(N_parts):      \n",
    "        # Calculate weight vector \"d\"\n",
    "        d = NNs[N](t_flat)\n",
    "        optimizers[N] = torch.optim.Adam(NNs[N].parameters(), lr = 0.01)\n",
    "\n",
    "        # Calculate attention map \"M\"\n",
    "        mi = attention_map(channels[0], d)\n",
    "        M.append(mi)\n",
    "    l_cng = torch.tensor(loss_CNG(M, coordinates))\n",
    "    \n",
    "    P = part_based_features(channels[0], M)\n",
    "    \n",
    "    output = nn.Softmax(dim=0)(vgg19.classifier(P.view(1,980)))\n",
    "    \n",
    "    loss = l_cng + loss_clf(output, torch.tensor([5]))\n",
    "    for i, l in enumerate(loss):\n",
    "        l.backward(retain_graph=True)\n",
    "    \n",
    "    for N in range(N_parts): \n",
    "        optimizers[N].step()\n",
    "        optimizers[N].zero_grad()\n",
    "        \n",
    "    print(loss,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = defaultdict(float)\n",
    "    counter = 0\n",
    "    for imgs, label in testloader:\n",
    "        for img in imgs:\n",
    "            optimizer = torch.optim.Adam(NNs[N].parameters(), lr = 0.01)\n",
    "            optimizer.zero_grad()\n",
    "            img = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(img/255) \n",
    "            img = img.unsqueeze(0)\n",
    "            channels = vgg19.features(img)\n",
    "            coordinates = list()\n",
    "            for channel in channels.detach().numpy()[0]:\n",
    "                coordinates = coordinates + [unravel_index(channel.argmax(), channel.shape)]\n",
    "            t_flat = [point for coordinate in coordinates for point in coordinate]\n",
    "            t_flat = torch.FloatTensor(t_flat)\n",
    "            M = list()\n",
    "            for N in range(N_parts):\n",
    "                # Calculate weight vector \"d\"\n",
    "                d = NNs[N](t_flat)\n",
    "\n",
    "                # Calculate attention map \"M\"\n",
    "                mi = attention_map(channels[0], d)\n",
    "                M.append(mi)\n",
    "            loss = loss_CNG(M, coordinates)\n",
    "            for i, l in enumerate(loss):\n",
    "                running_loss[i] += l\n",
    "                l.backward(retain_graph=True)\n",
    "            \n",
    "\n",
    "        \n",
    "            optimizer.step()\n",
    "            counter +=1\n",
    "    print([l/counter for l in running_loss.values()],\"running_loss: \",running_loss,\" counter: \",counter,\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0] += 1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_flat.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "input_channels = 1\n",
    "output_features = 6\n",
    "epoch = 1\n",
    "save_model_name = 'models/pretrained.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CUBDataset()\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=10, shuffle=True)\n",
    "testset = CUBDataset(is_test = True)\n",
    "testloader = DataLoader(dataset=testset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = torch.hub.load('pytorch/vision:v0.9.0', 'vgg19', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img , label = trainset[120]\n",
    "print(img.shape)\n",
    "print(img.transpose(1,2,0).shape)\n",
    "baseimage = Image.fromarray(img.transpose(1,2,0).astype(np.uint8))\n",
    "baseimage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.ToTensor()(img)\n",
    "img = img.permute(1,2,0)\n",
    "mean, std = img.mean([1,2]), img.std([1,2])\n",
    "#print(\"Mean:\", mean, \"\\nStd:\", std)\n",
    "#print(\"image shape:\", img.shape)\n",
    "#img = transforms.Normalize(mean, std)(img)\n",
    "#/255 -> Vgg19 Normalization\n",
    "img = img/255\n",
    "img = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(img) \n",
    "img = img.unsqueeze(0)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = nn.Softmax(dim=1)(vgg19(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"classes_vgg19.txt\",\"r\") as f:\n",
    "    classes = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_o = 0\n",
    "highscore = None\n",
    "for i, o in enumerate(output[0]):\n",
    "    if max_o < o:\n",
    "        highscore = i\n",
    "        max_o = o\n",
    "\n",
    "print(highscore,classes[highscore])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = vgg19.features(img)\n",
    "channels = channels.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square = 5\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(channels[0, ix-1, :, :], cmap='gray')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula 1: determine t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = list()\n",
    "for channel in channels[0]:\n",
    "    #print(np.max(channel))\n",
    "    t.append(unravel_index(channel.argmax(), channel.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = zip(*t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = list()\n",
    "y_list = list()\n",
    "for x,y in t:\n",
    "    x_list.append(x)\n",
    "    y_list.append(y)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_list,y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_xy = counter.most_common(20)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for xy,_ in part_xy:\n",
    "    x,y = xy\n",
    "    i = ImageDraw.Draw(baseimage).rectangle([(x-1)*32,(y-1)*32,x*32,y*32])\n",
    "baseimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = ImageDraw.Draw(baseimage)\n",
    "i.rectangle([1,1,32,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clustering(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Clustering, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 512)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_flatten = [i for xy in t for i in xy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_flatten= torch.Tensor(t_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19.fc1 = nn.Linear(in_features=1024, out_features=512, bias=True)\n",
    "vgg19.fc2 = nn.Linear(in_features=512, out_features=512, bias=True)\n",
    "#vgg19.fc(torch.Tensor(t).permute(1,0))\n",
    "vgg19.fc2(vgg19.fc1(t_flatten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Part(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Part, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_matrix = torch.clone(x)\n",
    "        conv_matrix = conv_matrix.reshape(conv_matrix.size(0), 512, 1, 784) #512 = patterns; 784 = 28x28 pattern w x h\n",
    "        conv_matrix = conv_matrix.transpose(1, 3)\n",
    "        x = F.avg_pool2d(x, kernel_size=28, stride=28)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x).unsqueeze(1).unsqueeze(1)\n",
    "        x = F.interpolate(x, (1, 784), mode='bilinear', align_corners=True)\n",
    "        x = x.squeeze(1).squeeze(1).unsqueeze(2).unsqueeze(3)\n",
    "        x = x * conv_matrix\n",
    "        x = F.avg_pool2d(x, kernel_size=(1, 512), stride=512)\n",
    "        x = x * 0.1\n",
    "        x = F.softmax(x, dim=1)\n",
    "        x = torch.exp(x)\n",
    "        x = x + 1\n",
    "        x = torch.log(x)\n",
    "        x = x * 4\n",
    "        x = x.squeeze(2).squeeze(2)\n",
    "        return x.reshape(x.size(0), 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Loss, self).__init__()\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        loss_sum = torch.zeros(1).cuda()\n",
    "        indexes = Loss.get_max_index(tensor)\n",
    "        for i in range(len(indexes)):\n",
    "            max_x, max_y = indexes[i]\n",
    "            for j in range(tensor.size(1)):\n",
    "                for k in range(tensor.size(2)):\n",
    "                    loss_sum += ((max_x - j) * (max_x - j) + (max_y - k) * (max_y - k)) * tensor[i, j, k]\n",
    "        return loss_sum\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_index(tensor):\n",
    "        shape = tensor.shape\n",
    "        indexes = []\n",
    "        for i in range(shape[0]):\n",
    "            mx = tensor[i, 0, 0]\n",
    "            x, y = 0, 0\n",
    "            for j in range(shape[1]):\n",
    "                for k in range(shape[2]):\n",
    "                    if tensor[i, j, k] > mx:\n",
    "                        mx = tensor[i, j, k]\n",
    "                        x, y = j, k\n",
    "            indexes.append([x, y])\n",
    "        return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = Part()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(part.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img , label = trainset[55]\n",
    "Image.fromarray(img.transpose(1,2,0).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.ToTensor()(img)\n",
    "img = img.permute(1,2,0)\n",
    "print(img.shape)\n",
    "mean, std = img.mean([1,2]), img.std([1,2])\n",
    "print(\"Mean:\", mean, \"\\nStd:\", std)\n",
    "img = img/255\n",
    "img = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(img)\n",
    "img = img.unsqueeze(0)\n",
    "print(\"image shape:\", img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = vgg19.features[0:36](img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"channel shape:\", channels.shape)\n",
    "output = part(channels)\n",
    "print(\"output shape:\", output.shape)\n",
    "optimizer.zero_grad()\n",
    "loss = loss_fn(output)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_model = torch.load(save_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = part_model(channels)\n",
    "x = output.permute(1,2,0).detach().numpy()\n",
    "print(x.shape)\n",
    "print(type(x))\n",
    "print(np.stack((x,x,x),axis=2).squeeze(-1).shape)\n",
    "Image.fromarray(np.stack((x,x,x),axis=2).squeeze(-1).astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "for epoch_number in range(epoch):\n",
    "    running_loss, count, acc = 0., 0, 0.\n",
    "    for batch, label in trainloader:\n",
    "        for img in batch:\n",
    "            t = time.time()\n",
    "            #print(img.shape)\n",
    "            img = img/255\n",
    "            img = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])(img)\n",
    "            img = img.unsqueeze(0)\n",
    "            #print(\"image shape:\", img.shape)\n",
    "            channels = vgg19.features[0:36](img)\n",
    "            #print(\"channel shape:\", channels.shape)\n",
    "            output = part(channels)\n",
    "            #print(\"output shape:\", output.shape)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            count += img.size(0)\n",
    "            #print(time.time() - t)\n",
    "        print(epoch_number, count, running_loss, Loss.get_max_index(output))\n",
    "\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(part, save_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
