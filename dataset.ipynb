{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(func):\n",
    "    \"\"\"flats a nested list to a normal list if the sublists have the lenght of 1\"\"\"\n",
    "    def wrapper(*args):\n",
    "        nested_list = func(*args)\n",
    "        if isinstance(nested_list[0], list) and len(nested_list[0]) == 1:\n",
    "            return [elm for sublist in nested_list for elm in sublist]\n",
    "        return nested_list\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "class CUBDataset(Dataset):\n",
    "    def __init__(self, seq_size: int, root: str = \"datasets/CUB_200_2011/\"):\n",
    "        self.seq_size = seq_size\n",
    "        self.root = root\n",
    "        images = self._read_file(\"images.txt\")\n",
    "        labels = self._read_file(\"image_class_labels.txt\", True)\n",
    "        train_test = self._read_file(\"train_test_split.txt\", True)\n",
    "        bounding_boxes = self._read_file(\"bounding_boxes.txt\", True)\n",
    "        \n",
    "        # check if all lists have the same lenght.\n",
    "        assert all(len(images) == len(l) for l in [labels, train_test, bounding_boxes])\n",
    "        \n",
    "        self.train = [(img, bb) for img, x, bb in zip(images, train_test, bounding_boxes) if x]\n",
    "        self.test = [(img, bb) for img, x, bb in zip(images, train_test, bounding_boxes) if not x]\n",
    "        \n",
    "        # check if training and test data is a smaller subset of the dataset.\n",
    "        assert all(len(images) > len(l) for l in [self.train, self.test])\n",
    "        \n",
    "        #TODO add image usage\n",
    "        \n",
    "        \n",
    "    @flatten\n",
    "    def _read_file(self, file: str, as_int: bool = False):\n",
    "        data = [line.split()[1:] for line in open(self.root + file)]\n",
    "        if as_int:\n",
    "            data = [[int(float(elm)) for elm in sublist] for sublist in data]\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train) + len(self.test)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "cub = CUBDataset(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"datasets/CUB_200_2011/\"\n",
    "paths_with_col_names = {\n",
    "    \"images.txt\": (\"image_id\", \"image_name\"),\n",
    "    \"train_test_split.txt\": (\"image_id\", \"is_training_image\"),\n",
    "    \"image_class_labels.txt\": (\"image_id\", \"class_id\"),\n",
    "    \"classes.txt\": (\"class_id\", \"class_name\"),\n",
    "    \"bounding_boxes.txt\": (\"image_id\", \"bb_x\", \"bb_y\", \"bb_width\", \"bb_height\"),\n",
    "    \"parts/part_locs.txt\": (\"image_id\", \"part_id\", \"p_x\", \"p_y\", \"p_visible\"),\n",
    "    \"parts/parts.txt\": (\"part_id\", \"part_name\"),\n",
    "    \"parts/part_click_locs.txt\": (\"image_id\", \"part_id\", \"p_mturk_x\", \"p_mturk_y\", \"p_mturk_visible\", \"p_mturk_time\"),\n",
    "    #\"attributes/attributes.txt\": (\"attribute_id\", \"attribute_name\"),\n",
    "    #\"attributes/certainties.txt\": (\"certainty_id\", \"certainty_name\"),\n",
    "    #\"attributes/image_attribute_labels.txt\": (\"image_id\", \"attribute_id\", \"att_is_present\", \"certainty_id\", \"att_time\", \"_1\", \"_2\"),\n",
    "    #\"attributes/class_attribute_labels_continuous.txt\": ()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath + \"attributes/image_attribute_labels.txt\", \"r\") as f:\n",
    "    x = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-184-7875e39b055e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-184-7875e39b055e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pd.DataFrame([i.split() for i in x]).loc[:,5].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(filepath + \"attributes/image_attribute_labels.txt\", sep=r\"(?<=\\d)\\s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "dataframes = [\n",
    "    pd.read_csv(filepath + path, sep=r\"(?<=\\d)\\s\", names=col_names)\n",
    "        .set_index([col for col in col_names if col.endswith(\"_id\")])\n",
    "    for path, col_names in paths.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reduce(lambda df1, df2: pd.merge(df1, df2, left_index=True, right_index=True), dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
